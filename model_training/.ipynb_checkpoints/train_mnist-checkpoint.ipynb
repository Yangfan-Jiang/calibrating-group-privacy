{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Learning with Group Privacy\n",
    "from MLModel import *\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scattering, K, (h, w) = get_scatter_transform()\n",
    "scattering.to(device)\n",
    "\n",
    "def get_scattered_feature(dataset):\n",
    "    scatters = []\n",
    "    targets = []\n",
    "    \n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=256, shuffle=True, num_workers=1, pin_memory=True)\n",
    "\n",
    "    \n",
    "    for (data, target) in loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        if scattering is not None:\n",
    "            data = scattering(data)\n",
    "        scatters.append(data)\n",
    "        targets.append(target)\n",
    "\n",
    "    scatters = torch.cat(scatters, axis=0)\n",
    "    targets = torch.cat(targets, axis=0)\n",
    "\n",
    "    data = torch.utils.data.TensorDataset(scatters, targets)\n",
    "    return data\n",
    "\n",
    "def load_mnist():\n",
    "    train = datasets.MNIST(root=\"~/data/\", train=True, download=True, transform=transforms.ToTensor())\n",
    "    test = datasets.MNIST(root=\"~/data/\", train=False, download=True, transform=transforms.ToTensor())\n",
    "    \n",
    "    # get scattered features\n",
    "    train = get_scattered_feature(train)\n",
    "    test = get_scattered_feature(test)\n",
    "    \n",
    "    train_data = train[:][0].squeeze().cpu().float()\n",
    "    train_label = train[:][1].cpu()\n",
    "    \n",
    "    test_data = test[:][0].squeeze().cpu().float()\n",
    "    test_label = test[:][1].cpu()\n",
    "\n",
    "    dataset = [(train_data, train_label), (test_data, test_label)]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#. training records= 60000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2321197/2464145456.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_data = torch.tensor(d[0][0]).to(device)\n",
      "/tmp/ipykernel_2321197/2464145456.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_label = torch.tensor(d[0][1]).to(device)\n",
      "/tmp/ipykernel_2321197/2464145456.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_data = torch.tensor(d[1][0]).to(device)\n",
      "/tmp/ipykernel_2321197/2464145456.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_label = torch.tensor(d[1][1]).to(device)\n"
     ]
    }
   ],
   "source": [
    "d = load_mnist()\n",
    "\n",
    "train_data = torch.tensor(d[0][0]).to(device)\n",
    "train_label = torch.tensor(d[0][1]).to(device)\n",
    "\n",
    "test_data = torch.tensor(d[1][0]).to(device)\n",
    "test_label = torch.tensor(d[1][1]).to(device)\n",
    "\n",
    "data_size = len(train_label)\n",
    "print(\"#. training records=\",data_size)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FL model parameters.\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "lr = 0.2\n",
    "\n",
    "configs = {\n",
    "    'output_size': 10,\n",
    "    'data_size': data_size,\n",
    "    'model': 'scatter',\n",
    "    'data': d,\n",
    "    'lr': lr,\n",
    "    'E': 500,\n",
    "    'delta': 1e-5,\n",
    "    'q': 0.05,\n",
    "    'clip': 0.1,\n",
    "    'batch_size': 128,\n",
    "    'device': device\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_acc(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    tot_sample = len(test_data)\n",
    "    #for i in range(len(test_data)):\n",
    "    \n",
    "    t_pred_y = model(test_data)\n",
    "    _, predicted = torch.max(t_pred_y, 1)\n",
    "    correct += (predicted == test_label).sum().item()\n",
    "    \n",
    "    acc = correct / tot_sample\n",
    "    return acc\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=configs['lr'], momentum=0.9)\n",
    "    # optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    start_time = time.time()\n",
    "    for e in range(configs['E']):\n",
    "        # randomly select q fraction samples from dataset by poisson sampling\n",
    "        idx = np.where(np.random.rand(len(torch_train[:][0])) < configs['q'])[0]\n",
    "\n",
    "        sampled_dataset = TensorDataset(torch_train[idx][0], torch_train[idx][1])\n",
    "        sample_data_loader = DataLoader(\n",
    "            dataset=sampled_dataset,\n",
    "            batch_size=configs['batch_size'],\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        clipped_grads = {name: torch.zeros_like(param) for name, param in model.named_parameters()}\n",
    "        for batch_x, batch_y in sample_data_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            pred_y = model(batch_x.float())\n",
    "            loss = criterion(pred_y, batch_y.long())\n",
    "\n",
    "            # bound l2 sensitivity (gradient clipping)\n",
    "            # clip each of the gradient in subset\n",
    "            for i in range(loss.size()[0]):\n",
    "                loss[i].backward(retain_graph=True)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=configs['clip'])\n",
    "                for name, param in model.named_parameters():\n",
    "                    clipped_grads[name] += param.grad \n",
    "                model.zero_grad()\n",
    "\n",
    "        # add Gaussian noise\n",
    "        for name, param in model.named_parameters():\n",
    "            clipped_grads[name] += gaussian_noise(clipped_grads[name].shape, configs['clip'], configs['sigma'], device=device)\n",
    "\n",
    "        # scale back\n",
    "        for name, param in model.named_parameters():\n",
    "            clipped_grads[name] /= (configs['data_size']*configs['q'])\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            param.grad = clipped_grads[name]\n",
    "\n",
    "        # update local model\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (e+1)%500 == 0:\n",
    "            acc = test_acc(model)\n",
    "            print(\"iters = {:d}, acc = {:.4f}\".format(e+1, acc), \" Time taken: %.2fs\" % (time.time() - start_time))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varying group size $m$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "noise scale is calibrated using binary search,\n",
    "see 'calibrate_sgm_noise.ipynb' file\n",
    "\"\"\"\n",
    "m_list = [8, 16, 32, 64]\n",
    "\n",
    "noise_rdp= [19.09490966796875, 46.65306091308594, 114.18656921386719, 279.62950134277344]\n",
    "noise_ours= [19.520643949508667, 31.144098043441772, 52.82178044319153, 94.88274216651917]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Naive RDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma = 19.09490966796875\n",
      "iters = 500, acc = 0.9647  Time taken: 543.84s\n",
      "iters = 500, acc = 0.9666  Time taken: 543.70s\n",
      "iters = 500, acc = 0.9655  Time taken: 545.07s\n",
      "iters = 500, acc = 0.9664  Time taken: 544.49s\n",
      "iters = 500, acc = 0.9663  Time taken: 542.79s\n",
      "avg acc = 0.9659000000000001\n",
      "=========\n",
      "sigma = 46.65306091308594\n",
      "iters = 500, acc = 0.9598  Time taken: 574.31s\n",
      "iters = 500, acc = 0.9580  Time taken: 587.57s\n",
      "iters = 500, acc = 0.9596  Time taken: 581.47s\n",
      "iters = 500, acc = 0.9552  Time taken: 622.36s\n",
      "iters = 500, acc = 0.9554  Time taken: 583.40s\n",
      "avg acc = 0.9576\n",
      "=========\n",
      "sigma = 114.18656921386719\n",
      "iters = 500, acc = 0.9065  Time taken: 589.02s\n",
      "iters = 500, acc = 0.9045  Time taken: 587.32s\n",
      "iters = 500, acc = 0.9026  Time taken: 584.90s\n",
      "iters = 500, acc = 0.9011  Time taken: 585.18s\n",
      "iters = 500, acc = 0.9100  Time taken: 585.35s\n",
      "avg acc = 0.9049400000000001\n",
      "=========\n",
      "sigma = 279.62950134277344\n",
      "iters = 500, acc = 0.6964  Time taken: 582.10s\n",
      "iters = 500, acc = 0.7433  Time taken: 588.59s\n",
      "iters = 500, acc = 0.7053  Time taken: 581.31s\n",
      "iters = 500, acc = 0.7256  Time taken: 601.53s\n",
      "iters = 500, acc = 0.7410  Time taken: 584.27s\n",
      "avg acc = 0.7223200000000001\n",
      "=========\n"
     ]
    }
   ],
   "source": [
    "for sigma in noise_rdp:\n",
    "    print(\"sigma =\",sigma)\n",
    "    configs['sigma'] = sigma\n",
    "    acc_list = []\n",
    "    for _ in range(5):\n",
    "        model = ScatterLinear(81, (7, 7), input_norm=\"GroupNorm\", num_groups=27).to(device)\n",
    "        torch_train = TensorDataset(torch.tensor(train_data), torch.tensor(train_label))\n",
    "        acc = train()\n",
    "        acc_list.append(acc)\n",
    "    print(\"avg acc =\", np.mean(acc_list))\n",
    "    print(\"=========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma = 19.520643949508667\n",
      "iters = 500, acc = 0.9663  Time taken: 584.24s\n",
      "iters = 500, acc = 0.9660  Time taken: 582.66s\n",
      "iters = 500, acc = 0.9648  Time taken: 586.00s\n",
      "iters = 500, acc = 0.9653  Time taken: 581.50s\n",
      "iters = 500, acc = 0.9662  Time taken: 585.43s\n",
      "avg acc = 0.9657199999999999\n",
      "=========\n",
      "sigma = 31.144098043441772\n",
      "iters = 500, acc = 0.9636  Time taken: 584.10s\n",
      "iters = 500, acc = 0.9642  Time taken: 586.90s\n",
      "iters = 500, acc = 0.9629  Time taken: 601.56s\n",
      "iters = 500, acc = 0.9635  Time taken: 614.25s\n",
      "iters = 500, acc = 0.9646  Time taken: 586.60s\n",
      "avg acc = 0.96376\n",
      "=========\n",
      "sigma = 52.82178044319153\n",
      "iters = 500, acc = 0.9524  Time taken: 590.84s\n",
      "iters = 500, acc = 0.9488  Time taken: 610.66s\n",
      "iters = 500, acc = 0.9534  Time taken: 581.29s\n",
      "iters = 500, acc = 0.9565  Time taken: 583.61s\n",
      "iters = 500, acc = 0.9556  Time taken: 615.20s\n",
      "avg acc = 0.9533400000000001\n",
      "=========\n",
      "sigma = 94.88274216651917\n",
      "iters = 500, acc = 0.9296  Time taken: 590.51s\n",
      "iters = 500, acc = 0.9259  Time taken: 591.78s\n",
      "iters = 500, acc = 0.9327  Time taken: 590.44s\n",
      "iters = 500, acc = 0.9199  Time taken: 589.57s\n",
      "iters = 500, acc = 0.9326  Time taken: 593.46s\n",
      "avg acc = 0.92814\n",
      "=========\n"
     ]
    }
   ],
   "source": [
    "for sigma in noise_ours:\n",
    "    print(\"sigma =\",sigma)\n",
    "    configs['sigma'] = sigma\n",
    "    acc_list = []\n",
    "    for _ in range(5):\n",
    "        model = ScatterLinear(81, (7, 7), input_norm=\"GroupNorm\", num_groups=27).to(device)\n",
    "        torch_train = TensorDataset(torch.tensor(train_data), torch.tensor(train_label))\n",
    "        acc = train()\n",
    "        acc_list.append(acc)\n",
    "    print(\"avg acc =\", np.mean(acc_list))\n",
    "    print(\"=========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vary privacy parameter $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "noise scale is calibrated using binary search,\n",
    "see 'calibrate_sgm_noise.ipynb' file\n",
    "\"\"\"\n",
    "# m = 32\n",
    "eps_list = [8, 7, 6, 5, 4, 3, 2]\n",
    "\n",
    "noise_rdp= [62.96098327636719, 70.53829956054688, 80.5775146484375, 94.09147644042969, 114.18656921386719, 147.5915069580078, 211.9739990234375]\n",
    "noise_ours= [29.204277992248535, 32.63718247413635, 37.39244818687439, 43.51409435272217, 52.82178044319153, 68.23326706886292, 97.94802665710449]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Naive RDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma = 62.96098327636719\n",
      "iters = 500, acc = 0.9527  Time taken: 584.57s\n",
      "iters = 500, acc = 0.9472  Time taken: 590.42s\n",
      "iters = 500, acc = 0.9500  Time taken: 591.47s\n",
      "iters = 500, acc = 0.9508  Time taken: 581.67s\n",
      "iters = 500, acc = 0.9500  Time taken: 616.31s\n",
      "avg acc = 0.95014\n",
      "=========\n",
      "sigma = 70.53829956054688\n",
      "iters = 500, acc = 0.9423  Time taken: 582.19s\n",
      "iters = 500, acc = 0.9400  Time taken: 593.37s\n",
      "iters = 500, acc = 0.9403  Time taken: 583.14s\n",
      "iters = 500, acc = 0.9420  Time taken: 607.90s\n",
      "iters = 500, acc = 0.9464  Time taken: 601.92s\n",
      "avg acc = 0.9421999999999999\n",
      "=========\n",
      "sigma = 80.5775146484375\n",
      "iters = 500, acc = 0.9298  Time taken: 597.55s\n",
      "iters = 500, acc = 0.9347  Time taken: 585.00s\n",
      "iters = 500, acc = 0.9393  Time taken: 582.45s\n",
      "iters = 500, acc = 0.9349  Time taken: 587.40s\n",
      "iters = 500, acc = 0.9356  Time taken: 582.44s\n",
      "avg acc = 0.9348599999999999\n",
      "=========\n",
      "sigma = 94.09147644042969\n",
      "iters = 500, acc = 0.9270  Time taken: 588.08s\n",
      "iters = 500, acc = 0.9251  Time taken: 584.26s\n",
      "iters = 500, acc = 0.9228  Time taken: 600.94s\n",
      "iters = 500, acc = 0.9274  Time taken: 588.83s\n",
      "iters = 500, acc = 0.9213  Time taken: 584.11s\n",
      "avg acc = 0.92472\n",
      "=========\n",
      "sigma = 114.18656921386719\n",
      "iters = 500, acc = 0.9121  Time taken: 672.17s\n",
      "iters = 500, acc = 0.8982  Time taken: 587.22s\n",
      "iters = 500, acc = 0.9103  Time taken: 582.91s\n",
      "iters = 500, acc = 0.9072  Time taken: 683.42s\n",
      "iters = 500, acc = 0.8960  Time taken: 687.91s\n",
      "avg acc = 0.9047600000000001\n",
      "=========\n",
      "sigma = 147.5915069580078\n",
      "iters = 500, acc = 0.8656  Time taken: 920.35s\n",
      "iters = 500, acc = 0.8762  Time taken: 1443.09s\n",
      "iters = 500, acc = 0.8668  Time taken: 836.80s\n",
      "iters = 500, acc = 0.8832  Time taken: 900.16s\n",
      "iters = 500, acc = 0.8721  Time taken: 584.33s\n",
      "avg acc = 0.87278\n",
      "=========\n",
      "sigma = 211.9739990234375\n",
      "iters = 500, acc = 0.8002  Time taken: 2300.39s\n",
      "iters = 500, acc = 0.8259  Time taken: 2093.63s\n",
      "iters = 500, acc = 0.8163  Time taken: 579.12s\n",
      "iters = 500, acc = 0.7885  Time taken: 581.34s\n",
      "iters = 500, acc = 0.7779  Time taken: 623.09s\n",
      "avg acc = 0.80176\n",
      "=========\n"
     ]
    }
   ],
   "source": [
    "for sigma in noise_rdp:\n",
    "    print(\"sigma =\",sigma)\n",
    "    configs['sigma'] = sigma\n",
    "    acc_list = []\n",
    "    for _ in range(5):\n",
    "        model = ScatterLinear(81, (7, 7), input_norm=\"GroupNorm\", num_groups=27).to(device)\n",
    "        torch_train = TensorDataset(torch.tensor(train_data), torch.tensor(train_label))\n",
    "        acc = train()\n",
    "        acc_list.append(acc)\n",
    "    print(\"avg acc =\", np.mean(acc_list))\n",
    "    print(\"=========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma = 29.204277992248535\n",
      "iters = 500, acc = 0.9663  Time taken: 579.75s\n",
      "iters = 500, acc = 0.9646  Time taken: 583.89s\n",
      "iters = 500, acc = 0.9629  Time taken: 578.44s\n",
      "iters = 500, acc = 0.9618  Time taken: 586.54s\n",
      "iters = 500, acc = 0.9643  Time taken: 580.99s\n",
      "avg acc = 0.96398\n",
      "=========\n",
      "sigma = 32.63718247413635\n",
      "iters = 500, acc = 0.9637  Time taken: 585.39s\n",
      "iters = 500, acc = 0.9633  Time taken: 582.10s\n",
      "iters = 500, acc = 0.9606  Time taken: 583.10s\n",
      "iters = 500, acc = 0.9648  Time taken: 588.85s\n",
      "iters = 500, acc = 0.9606  Time taken: 588.20s\n",
      "avg acc = 0.9625999999999999\n",
      "=========\n",
      "sigma = 37.39244818687439\n",
      "iters = 500, acc = 0.9598  Time taken: 580.22s\n",
      "iters = 500, acc = 0.9623  Time taken: 590.71s\n",
      "iters = 500, acc = 0.9638  Time taken: 587.65s\n",
      "iters = 500, acc = 0.9594  Time taken: 580.07s\n",
      "iters = 500, acc = 0.9635  Time taken: 581.92s\n",
      "avg acc = 0.96176\n",
      "=========\n",
      "sigma = 43.51409435272217\n",
      "iters = 500, acc = 0.9575  Time taken: 578.98s\n",
      "iters = 500, acc = 0.9564  Time taken: 581.03s\n",
      "iters = 500, acc = 0.9578  Time taken: 576.90s\n",
      "iters = 500, acc = 0.9549  Time taken: 579.99s\n",
      "iters = 500, acc = 0.9591  Time taken: 583.75s\n",
      "avg acc = 0.9571399999999999\n",
      "=========\n",
      "sigma = 52.82178044319153\n",
      "iters = 500, acc = 0.9572  Time taken: 588.23s\n",
      "iters = 500, acc = 0.9571  Time taken: 581.49s\n",
      "iters = 500, acc = 0.9543  Time taken: 583.60s\n"
     ]
    }
   ],
   "source": [
    "for sigma in noise_ours:\n",
    "    print(\"sigma =\",sigma)\n",
    "    configs['sigma'] = sigma\n",
    "    acc_list = []\n",
    "    for _ in range(5):\n",
    "        model = ScatterLinear(81, (7, 7), input_norm=\"GroupNorm\", num_groups=27).to(device)\n",
    "        torch_train = TensorDataset(torch.tensor(train_data), torch.tensor(train_label))\n",
    "        acc = train()\n",
    "        acc_list.append(acc)\n",
    "    print(\"avg acc =\", np.mean(acc_list))\n",
    "    print(\"=========\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
