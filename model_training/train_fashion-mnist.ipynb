{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Learning with Group Privacy\n",
    "from MLModel import *\n",
    "\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "import time\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "#device = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "scattering, K, (h, w) = get_scatter_transform()\n",
    "scattering.to(device)\n",
    "\n",
    "def get_scattered_feature(dataset):\n",
    "    scatters = []\n",
    "    targets = []\n",
    "    \n",
    "    loader = torch.utils.data.DataLoader(\n",
    "        dataset, batch_size=256, shuffle=True, num_workers=1, pin_memory=True)\n",
    "\n",
    "    \n",
    "    for (data, target) in loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "        if scattering is not None:\n",
    "            data = scattering(data)\n",
    "        scatters.append(data)\n",
    "        targets.append(target)\n",
    "\n",
    "    scatters = torch.cat(scatters, axis=0)\n",
    "    targets = torch.cat(targets, axis=0)\n",
    "\n",
    "    data = torch.utils.data.TensorDataset(scatters, targets)\n",
    "    return data\n",
    "\n",
    "def load_mnist():\n",
    "    train = datasets.FashionMNIST(root=\"~/data/\", train=True, download=True, transform=transforms.ToTensor())\n",
    "    test = datasets.FashionMNIST(root=\"~/data/\", train=False, download=True, transform=transforms.ToTensor())\n",
    "    \n",
    "    # get scattered features\n",
    "    train = get_scattered_feature(train)\n",
    "    test = get_scattered_feature(test)\n",
    "    \n",
    "    train_data = train[:][0].squeeze().cpu().float()\n",
    "    train_label = train[:][1].cpu()\n",
    "    \n",
    "    test_data = test[:][0].squeeze().cpu().float()\n",
    "    test_label = test[:][1].cpu()\n",
    "\n",
    "    dataset = [(train_data, train_label), (test_data, test_label)]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2321299/2464145456.py:3: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_data = torch.tensor(d[0][0]).to(device)\n",
      "/tmp/ipykernel_2321299/2464145456.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_label = torch.tensor(d[0][1]).to(device)\n",
      "/tmp/ipykernel_2321299/2464145456.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_data = torch.tensor(d[1][0]).to(device)\n",
      "/tmp/ipykernel_2321299/2464145456.py:7: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_label = torch.tensor(d[1][1]).to(device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#. training records= 60000\n"
     ]
    }
   ],
   "source": [
    "d = load_mnist()\n",
    "\n",
    "train_data = torch.tensor(d[0][0]).to(device)\n",
    "train_label = torch.tensor(d[0][1]).to(device)\n",
    "\n",
    "test_data = torch.tensor(d[1][0]).to(device)\n",
    "test_label = torch.tensor(d[1][1]).to(device)\n",
    "\n",
    "data_size = len(train_label)\n",
    "print(\"#. training records=\",data_size)\n",
    "\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "FL model parameters.\n",
    "\"\"\"\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "lr = 0.2\n",
    "\n",
    "configs = {\n",
    "    'output_size': 10,\n",
    "    'data_size': data_size,\n",
    "    'model': 'scatter',\n",
    "    'data': d,\n",
    "    'lr': lr,\n",
    "    'E': 500,\n",
    "    'delta': 1e-5,\n",
    "    'q': 0.05,\n",
    "    'clip': 0.1,\n",
    "    'batch_size': 128,\n",
    "    'device': device\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def test_acc(model):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    tot_sample = len(test_data)\n",
    "    #for i in range(len(test_data)):\n",
    "    \n",
    "    t_pred_y = model(test_data)\n",
    "    _, predicted = torch.max(t_pred_y, 1)\n",
    "    correct += (predicted == test_label).sum().item()\n",
    "    \n",
    "    acc = correct / tot_sample\n",
    "    return acc\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    criterion = nn.CrossEntropyLoss(reduction='none')\n",
    "    optimizer = torch.optim.SGD(model.parameters(), lr=configs['lr'], momentum=0.9)\n",
    "    # optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "    acc = []\n",
    "    start_time = time.time()\n",
    "    for e in range(configs['E']):\n",
    "        # randomly select q fraction samples from dataset by poisson sampling\n",
    "        idx = np.where(np.random.rand(len(torch_train[:][0])) < configs['q'])[0]\n",
    "\n",
    "        sampled_dataset = TensorDataset(torch_train[idx][0], torch_train[idx][1])\n",
    "        sample_data_loader = DataLoader(\n",
    "            dataset=sampled_dataset,\n",
    "            batch_size=configs['batch_size'],\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        clipped_grads = {name: torch.zeros_like(param) for name, param in model.named_parameters()}\n",
    "        for batch_x, batch_y in sample_data_loader:\n",
    "            batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "            pred_y = model(batch_x.float())\n",
    "            loss = criterion(pred_y, batch_y.long())\n",
    "\n",
    "            # bound l2 sensitivity (gradient clipping)\n",
    "            # clip each of the gradient in subset\n",
    "            for i in range(loss.size()[0]):\n",
    "                loss[i].backward(retain_graph=True)\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=configs['clip'])\n",
    "                for name, param in model.named_parameters():\n",
    "                    clipped_grads[name] += param.grad \n",
    "                model.zero_grad()\n",
    "\n",
    "        # add Gaussian noise\n",
    "        for name, param in model.named_parameters():\n",
    "            clipped_grads[name] += gaussian_noise(clipped_grads[name].shape, configs['clip'], configs['sigma'], device=device)\n",
    "\n",
    "        # scale back\n",
    "        for name, param in model.named_parameters():\n",
    "            clipped_grads[name] /= (configs['data_size']*configs['q'])\n",
    "\n",
    "        for name, param in model.named_parameters():\n",
    "            param.grad = clipped_grads[name]\n",
    "\n",
    "        # update local model\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (e+1)%500 == 0:\n",
    "            acc = test_acc(model)\n",
    "            print(\"iters = {:d}, acc = {:.4f}\".format(e+1, acc), \" Time taken: %.2fs\" % (time.time() - start_time))\n",
    "    return acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Varying group size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "noise scale is calibrated using binary search,\n",
    "see 'calibrate_sgm_noise.ipynb' file\n",
    "\"\"\"\n",
    "m_list = [8, 16, 32, 64]\n",
    "\n",
    "noise_rdp= [19.09490966796875, 46.65306091308594, 114.18656921386719, 279.62950134277344]\n",
    "noise_ours= [19.520643949508667, 31.144098043441772, 52.82178044319153, 94.88274216651917]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Naive RDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma = 19.09490966796875\n",
      "iters = 500, acc = 0.8331  Time taken: 530.71s\n",
      "iters = 500, acc = 0.8337  Time taken: 532.15s\n",
      "iters = 500, acc = 0.8359  Time taken: 531.46s\n",
      "iters = 500, acc = 0.8326  Time taken: 528.40s\n",
      "iters = 500, acc = 0.8362  Time taken: 529.68s\n",
      "avg acc = 0.8343\n",
      "=========\n",
      "sigma = 46.65306091308594\n",
      "iters = 500, acc = 0.8238  Time taken: 544.61s\n",
      "iters = 500, acc = 0.8201  Time taken: 560.29s\n",
      "iters = 500, acc = 0.8213  Time taken: 571.26s\n",
      "iters = 500, acc = 0.8234  Time taken: 566.83s\n",
      "iters = 500, acc = 0.8225  Time taken: 534.71s\n",
      "avg acc = 0.82222\n",
      "=========\n",
      "sigma = 114.18656921386719\n",
      "iters = 500, acc = 0.7653  Time taken: 540.69s\n",
      "iters = 500, acc = 0.7642  Time taken: 610.75s\n",
      "iters = 500, acc = 0.7698  Time taken: 587.65s\n",
      "iters = 500, acc = 0.7654  Time taken: 608.78s\n",
      "iters = 500, acc = 0.7641  Time taken: 602.67s\n",
      "avg acc = 0.76576\n",
      "=========\n",
      "sigma = 279.62950134277344\n",
      "iters = 500, acc = 0.6484  Time taken: 606.35s\n",
      "iters = 500, acc = 0.6075  Time taken: 578.34s\n",
      "iters = 500, acc = 0.6096  Time taken: 535.59s\n",
      "iters = 500, acc = 0.6467  Time taken: 528.81s\n",
      "iters = 500, acc = 0.6281  Time taken: 530.05s\n",
      "avg acc = 0.62806\n",
      "=========\n"
     ]
    }
   ],
   "source": [
    "for sigma in noise_rdp:\n",
    "    print(\"sigma =\",sigma)\n",
    "    configs['sigma'] = sigma\n",
    "    acc_list = []\n",
    "    for _ in range(5):\n",
    "        model = ScatterLinear(81, (7, 7), input_norm=\"GroupNorm\", num_groups=81).to(device)\n",
    "        torch_train = TensorDataset(torch.tensor(train_data), torch.tensor(train_label))\n",
    "        acc = train()\n",
    "        acc_list.append(acc)\n",
    "    print(\"avg acc =\", np.mean(acc_list))\n",
    "    print(\"=========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma = 19.520643949508667\n",
      "iters = 500, acc = 0.8305  Time taken: 548.06s\n",
      "iters = 500, acc = 0.8369  Time taken: 528.79s\n",
      "iters = 500, acc = 0.8361  Time taken: 649.17s\n",
      "iters = 500, acc = 0.8357  Time taken: 1697.89s\n",
      "iters = 500, acc = 0.8338  Time taken: 536.41s\n",
      "avg acc = 0.8346\n",
      "=========\n",
      "sigma = 31.144098043441772\n",
      "iters = 500, acc = 0.8315  Time taken: 533.71s\n",
      "iters = 500, acc = 0.8262  Time taken: 539.53s\n",
      "iters = 500, acc = 0.8320  Time taken: 542.28s\n",
      "iters = 500, acc = 0.8286  Time taken: 536.33s\n",
      "iters = 500, acc = 0.8290  Time taken: 542.74s\n",
      "avg acc = 0.8294599999999999\n",
      "=========\n",
      "sigma = 52.82178044319153\n",
      "iters = 500, acc = 0.8197  Time taken: 553.88s\n",
      "iters = 500, acc = 0.8119  Time taken: 542.46s\n",
      "iters = 500, acc = 0.8178  Time taken: 527.32s\n",
      "iters = 500, acc = 0.8198  Time taken: 539.66s\n",
      "iters = 500, acc = 0.8145  Time taken: 549.01s\n",
      "avg acc = 0.8167399999999999\n",
      "=========\n",
      "sigma = 94.88274216651917\n",
      "iters = 500, acc = 0.7752  Time taken: 536.70s\n",
      "iters = 500, acc = 0.7860  Time taken: 532.26s\n",
      "iters = 500, acc = 0.7933  Time taken: 532.46s\n",
      "iters = 500, acc = 0.7908  Time taken: 534.97s\n",
      "iters = 500, acc = 0.7794  Time taken: 533.38s\n",
      "avg acc = 0.78494\n",
      "=========\n"
     ]
    }
   ],
   "source": [
    "for sigma in noise_ours:\n",
    "    print(\"sigma =\",sigma)\n",
    "    configs['sigma'] = sigma\n",
    "    acc_list = []\n",
    "    for _ in range(5):\n",
    "        model = ScatterLinear(81, (7, 7), input_norm=\"GroupNorm\", num_groups=81).to(device)\n",
    "        torch_train = TensorDataset(torch.tensor(train_data), torch.tensor(train_label))\n",
    "        acc = train()\n",
    "        acc_list.append(acc)\n",
    "    print(\"avg acc =\", np.mean(acc_list))\n",
    "    print(\"=========\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Varying privacy parameter $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "noise scale is calibrated using binary search,\n",
    "see 'calibrate_sgm_noise.ipynb' file\n",
    "\"\"\"\n",
    "# m = 32\n",
    "eps_list = [8, 7, 6, 5, 4, 3, 2]\n",
    "\n",
    "noise_rdp= [62.96098327636719, 70.53829956054688, 80.5775146484375, 94.09147644042969, 114.18656921386719, 147.5915069580078, 211.9739990234375]\n",
    "noise_ours= [29.204277992248535, 32.63718247413635, 37.39244818687439, 43.51409435272217, 52.82178044319153, 68.23326706886292, 97.94802665710449]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naive RDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma = 62.96098327636719\n",
      "iters = 500, acc = 0.8139  Time taken: 532.04s\n",
      "iters = 500, acc = 0.8136  Time taken: 528.60s\n",
      "iters = 500, acc = 0.8071  Time taken: 527.33s\n",
      "iters = 500, acc = 0.8131  Time taken: 564.92s\n",
      "iters = 500, acc = 0.8091  Time taken: 537.66s\n",
      "avg acc = 0.81136\n",
      "=========\n",
      "sigma = 70.53829956054688\n",
      "iters = 500, acc = 0.8049  Time taken: 531.37s\n",
      "iters = 500, acc = 0.8034  Time taken: 532.73s\n",
      "iters = 500, acc = 0.8056  Time taken: 538.73s\n",
      "iters = 500, acc = 0.8011  Time taken: 530.53s\n",
      "iters = 500, acc = 0.8029  Time taken: 529.56s\n",
      "avg acc = 0.80358\n",
      "=========\n",
      "sigma = 80.5775146484375\n",
      "iters = 500, acc = 0.7942  Time taken: 528.55s\n",
      "iters = 500, acc = 0.7988  Time taken: 532.27s\n",
      "iters = 500, acc = 0.7975  Time taken: 529.44s\n",
      "iters = 500, acc = 0.7974  Time taken: 531.10s\n",
      "iters = 500, acc = 0.7985  Time taken: 533.55s\n",
      "avg acc = 0.79728\n",
      "=========\n",
      "sigma = 94.09147644042969\n",
      "iters = 500, acc = 0.7802  Time taken: 533.54s\n",
      "iters = 500, acc = 0.7833  Time taken: 528.94s\n",
      "iters = 500, acc = 0.7887  Time taken: 530.06s\n",
      "iters = 500, acc = 0.7810  Time taken: 530.75s\n",
      "iters = 500, acc = 0.7855  Time taken: 531.29s\n",
      "avg acc = 0.78374\n",
      "=========\n",
      "sigma = 114.18656921386719\n",
      "iters = 500, acc = 0.7556  Time taken: 528.79s\n",
      "iters = 500, acc = 0.7655  Time taken: 560.84s\n",
      "iters = 500, acc = 0.7591  Time taken: 613.83s\n",
      "iters = 500, acc = 0.7655  Time taken: 580.37s\n",
      "iters = 500, acc = 0.7612  Time taken: 531.83s\n",
      "avg acc = 0.7613800000000001\n",
      "=========\n",
      "sigma = 147.5915069580078\n",
      "iters = 500, acc = 0.7373  Time taken: 539.07s\n",
      "iters = 500, acc = 0.7398  Time taken: 541.22s\n",
      "iters = 500, acc = 0.7313  Time taken: 596.28s\n",
      "iters = 500, acc = 0.7372  Time taken: 836.65s\n",
      "iters = 500, acc = 0.7475  Time taken: 684.80s\n",
      "avg acc = 0.73862\n",
      "=========\n",
      "sigma = 211.9739990234375\n",
      "iters = 500, acc = 0.6811  Time taken: 1440.42s\n",
      "iters = 500, acc = 0.7004  Time taken: 1007.95s\n",
      "iters = 500, acc = 0.6631  Time taken: 1214.57s\n",
      "iters = 500, acc = 0.6772  Time taken: 1447.11s\n",
      "iters = 500, acc = 0.6949  Time taken: 670.54s\n",
      "avg acc = 0.6833400000000001\n",
      "=========\n"
     ]
    }
   ],
   "source": [
    "for sigma in noise_rdp:\n",
    "    print(\"sigma =\",sigma)\n",
    "    configs['sigma'] = sigma\n",
    "    acc_list = []\n",
    "    for _ in range(5):\n",
    "        model = ScatterLinear(81, (7, 7), input_norm=\"GroupNorm\", num_groups=81).to(device)\n",
    "        torch_train = TensorDataset(torch.tensor(train_data), torch.tensor(train_label))\n",
    "        acc = train()\n",
    "        acc_list.append(acc)\n",
    "    print(\"avg acc =\", np.mean(acc_list))\n",
    "    print(\"=========\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sigma = 29.204277992248535\n",
      "iters = 500, acc = 0.8284  Time taken: 1755.45s\n",
      "iters = 500, acc = 0.8328  Time taken: 573.85s\n",
      "iters = 500, acc = 0.8305  Time taken: 572.50s\n",
      "iters = 500, acc = 0.8296  Time taken: 582.63s\n",
      "iters = 500, acc = 0.8316  Time taken: 572.12s\n",
      "avg acc = 0.83058\n",
      "=========\n",
      "sigma = 32.63718247413635\n",
      "iters = 500, acc = 0.8295  Time taken: 573.74s\n",
      "iters = 500, acc = 0.8330  Time taken: 574.27s\n",
      "iters = 500, acc = 0.8259  Time taken: 573.11s\n",
      "iters = 500, acc = 0.8297  Time taken: 585.12s\n",
      "iters = 500, acc = 0.8272  Time taken: 583.53s\n",
      "avg acc = 0.8290599999999999\n",
      "=========\n",
      "sigma = 37.39244818687439\n",
      "iters = 500, acc = 0.8300  Time taken: 571.59s\n",
      "iters = 500, acc = 0.8295  Time taken: 572.04s\n",
      "iters = 500, acc = 0.8293  Time taken: 578.68s\n",
      "iters = 500, acc = 0.8278  Time taken: 602.71s\n",
      "iters = 500, acc = 0.8254  Time taken: 584.43s\n",
      "avg acc = 0.8283999999999999\n",
      "=========\n",
      "sigma = 43.51409435272217\n",
      "iters = 500, acc = 0.8263  Time taken: 607.63s\n",
      "iters = 500, acc = 0.8169  Time taken: 573.30s\n",
      "iters = 500, acc = 0.8239  Time taken: 578.57s\n",
      "iters = 500, acc = 0.8236  Time taken: 581.04s\n",
      "iters = 500, acc = 0.8314  Time taken: 582.28s\n",
      "avg acc = 0.8244199999999999\n",
      "=========\n",
      "sigma = 52.82178044319153\n",
      "iters = 500, acc = 0.8142  Time taken: 574.24s\n",
      "iters = 500, acc = 0.8213  Time taken: 575.00s\n",
      "iters = 500, acc = 0.8135  Time taken: 574.66s\n",
      "iters = 500, acc = 0.8183  Time taken: 571.81s\n",
      "iters = 500, acc = 0.8184  Time taken: 572.69s\n",
      "avg acc = 0.8171399999999999\n",
      "=========\n",
      "sigma = 68.23326706886292\n",
      "iters = 500, acc = 0.8021  Time taken: 574.54s\n",
      "iters = 500, acc = 0.8060  Time taken: 577.37s\n",
      "iters = 500, acc = 0.8094  Time taken: 578.54s\n"
     ]
    }
   ],
   "source": [
    "for sigma in noise_ours:\n",
    "    print(\"sigma =\",sigma)\n",
    "    configs['sigma'] = sigma\n",
    "    acc_list = []\n",
    "    for _ in range(5):\n",
    "        model = ScatterLinear(81, (7, 7), input_norm=\"GroupNorm\", num_groups=81).to(device)\n",
    "        torch_train = TensorDataset(torch.tensor(train_data), torch.tensor(train_label))\n",
    "        acc = train()\n",
    "        acc_list.append(acc)\n",
    "    print(\"avg acc =\", np.mean(acc_list))\n",
    "    print(\"=========\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
